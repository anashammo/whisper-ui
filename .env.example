# Whisper Transcription API - Environment Configuration
# Copy this file to .env and configure as needed
# Most settings have sensible defaults and don't need to be changed
#
# ============================================================================
# NOTE: Environment files have been reorganized for better separation
# ============================================================================
# - Backend config: src/presentation/api/.env.example (recommended)
# - Frontend config: src/presentation/frontend/src/environments/environment.ts
# - This root-level file is maintained for backward compatibility
# ============================================================================

# ============================================================================
# Application Settings
# ============================================================================

# Application name and version (used in API responses)
APP_NAME=Whisper Transcription API
APP_VERSION=1.0.0

# ============================================================================
# API Server Configuration
# ============================================================================

# Host to bind the API server to
# - 0.0.0.0: Listen on all network interfaces (default)
# - localhost or 127.0.0.1: Listen only on local machine
API_HOST=0.0.0.0

# Port for the API server
# Default: 8001 (NOT 8000, to avoid conflicts)
API_PORT=8001

# Logging level
# Options: debug, info, warning, error, critical
LOG_LEVEL=info

# ============================================================================
# Whisper Model Configuration
# ============================================================================

# Default Whisper model to use for transcriptions
# Options: tiny, base, small, medium, large, turbo
#
# Model Comparison:
# - tiny:   ~75MB  download, ~1GB  VRAM (fastest, least accurate)
# - base:   ~150MB download, ~1GB  VRAM (recommended, good balance)
# - small:  ~500MB download, ~2GB  VRAM (better accuracy)
# - medium: ~1.5GB download, ~5GB  VRAM (high accuracy)
# - large:  ~3GB   download, ~10GB VRAM (best accuracy, slowest)
# - turbo:  ~3GB   download, ~6GB  VRAM (speed + accuracy optimized)
WHISPER_MODEL=base

# Device to run Whisper model on
# Options: cuda, cpu
# - cuda: Use GPU acceleration (requires NVIDIA GPU with CUDA)
# - cpu: Use CPU (slower, but works without GPU)
WHISPER_DEVICE=cuda

# ============================================================================
# Database Configuration
# ============================================================================

# Database connection URL
# Default uses SQLite with local file
# Format: sqlite:///<path-to-database>
# Example: sqlite:///./whisper_transcriptions.db
DATABASE_URL=sqlite:///./whisper_transcriptions.db

# For production, consider PostgreSQL:
# DATABASE_URL=postgresql://user:password@localhost/whisper_db

# ============================================================================
# File Upload Settings
# ============================================================================

# Directory to store uploaded audio files
# Relative to project root
UPLOAD_DIR=./uploads

# Maximum file size for uploads (in MB)
# Default: 25MB
MAX_FILE_SIZE_MB=25

# Maximum audio duration (in seconds)
# Default: 30 seconds
# Note: Longer audio requires more VRAM and processing time
MAX_DURATION_SECONDS=30

# ============================================================================
# CORS (Cross-Origin Resource Sharing) Configuration
# ============================================================================

# Allowed origins for CORS
# Default allows Angular frontend on localhost:4200
# For production, specify your frontend domain
# Format: Comma-separated list or JSON array
CORS_ORIGINS=["http://localhost:4200","http://localhost:3000","http://localhost:8080"]

# For production (example):
# CORS_ORIGINS=["https://yourdomain.com","https://app.yourdomain.com"]

# Allow all origins (NOT recommended for production):
# CORS_ORIGINS=["*"]

# ============================================================================
# LLM Enhancement Configuration (Optional)
# ============================================================================

# Base URL for local LLM API (OpenAI-compatible)
# Default: Ollama default endpoint
# For LM Studio, use: http://localhost:1234/v1
LLM_BASE_URL=http://localhost:11434/v1

# LLM model name to use for transcription enhancement
# For Ollama: llama3, mistral, phi, etc.
# For LM Studio: model name as shown in LM Studio
LLM_MODEL=llama3

# Timeout for LLM requests (in seconds)
# Default: 60 seconds
# Increase for slower models or longer transcriptions
LLM_TIMEOUT_SECONDS=60

# LLM temperature (0.0 - 1.0)
# Lower = more focused and deterministic
# Higher = more creative and varied
# Default: 0.3 (good for transcription enhancement)
LLM_TEMPERATURE=0.3

# ============================================================================
# Ngrok Tunnel Configuration (Docker only)
# ============================================================================

# Ngrok auth token for tunnel authentication
# Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken
# Required when using --ngrok flag with docker scripts
NGROK_AUTHTOKEN=your_ngrok_authtoken_here

# Ngrok tunnels expose the following URLs (requires ngrok account with reserved domains):
# - Backend:  https://anas-hammo-whisper-backend.ngrok.dev
# - Frontend: https://anas-hammo-whisper-frontend.ngrok.dev
# - LLM:      https://anas-hammo-whisper-llm.ngrok.dev
#
# Ngrok Web Inspection UI (for debugging):
# - Backend:  http://localhost:4050
# - Frontend: http://localhost:4051
# - LLM:      http://localhost:4052

# ============================================================================
# Advanced Settings (Usually don't need to change)
# ============================================================================

# Enable debug mode (provides detailed error messages)
# WARNING: Only use in development, not in production
# DEBUG=false

# Number of worker processes for uvicorn
# Default: 1 (increase for production)
# WORKERS=1

# ============================================================================
# Notes
# ============================================================================
#
# 1. After changing .env, restart the backend server for changes to take effect
#
# 2. To use GPU acceleration:
#    - Ensure NVIDIA GPU drivers are installed
#    - Install CUDA toolkit (11.8 or higher)
#    - Install PyTorch with CUDA support
#    - Set WHISPER_DEVICE=cuda
#
# 3. Model downloads:
#    - Models are cached in ~/.cache/whisper/
#    - First use of a model will download it automatically
#    - Pre-download with: python scripts/download_whisper_model.py <model>
#
# 4. Database initialization:
#    - Run: python scripts/init_db.py
#    - Safe to run multiple times (won't delete data)
#
# 5. Frontend configuration:
#    - Frontend expects backend on port 8001
#    - If you change API_PORT, update frontend environment.ts
#    - Path: src/presentation/frontend/src/environments/environment.ts
