# Whisper Transcription API - Environment Configuration
# Copy this file to .env and configure as needed
# Most settings have sensible defaults and don't need to be changed

# ============================================================================
# Application Settings
# ============================================================================

# Application name and version (used in API responses)
APP_NAME=Whisper Transcription API
APP_VERSION=1.0.0

# ============================================================================
# API Server Configuration
# ============================================================================

# Host to bind the API server to
# - 0.0.0.0: Listen on all network interfaces (default)
# - localhost or 127.0.0.1: Listen only on local machine
API_HOST=0.0.0.0

# Port for the API server
# Default: 8001 (NOT 8000, to avoid conflicts)
API_PORT=8001

# Logging level
# Options: debug, info, warning, error, critical
LOG_LEVEL=info

# ============================================================================
# Whisper Model Configuration
# ============================================================================

# Default Whisper model to use for transcriptions
# Options: tiny, base, small, medium, large, turbo
#
# Model Comparison:
# - tiny:   ~75MB  download, ~1GB  VRAM (fastest, least accurate)
# - base:   ~150MB download, ~1GB  VRAM (recommended, good balance)
# - small:  ~500MB download, ~2GB  VRAM (better accuracy)
# - medium: ~1.5GB download, ~5GB  VRAM (high accuracy)
# - large:  ~3GB   download, ~10GB VRAM (best accuracy, slowest)
# - turbo:  ~3GB   download, ~6GB  VRAM (speed + accuracy optimized)
WHISPER_MODEL=base

# Device to run Whisper model on
# Options: cuda, cpu
# - cuda: Use GPU acceleration (requires NVIDIA GPU with CUDA)
# - cpu: Use CPU (slower, but works without GPU)
WHISPER_DEVICE=cuda

# ============================================================================
# Database Configuration
# ============================================================================

# Database connection URL
# Default uses SQLite with local file
# Format: sqlite:///<path-to-database>
# Example: sqlite:///./whisper_transcriptions.db
DATABASE_URL=sqlite:///./whisper_transcriptions.db

# For production, consider PostgreSQL:
# DATABASE_URL=postgresql://user:password@localhost/whisper_db

# ============================================================================
# File Upload Settings
# ============================================================================

# Directory to store uploaded audio files
# Relative to project root
UPLOAD_DIR=./uploads

# Maximum file size for uploads (in MB)
# Default: 25MB
MAX_FILE_SIZE_MB=25

# Maximum audio duration (in seconds)
# Default: 30 seconds
# Note: Longer audio requires more VRAM and processing time
MAX_DURATION_SECONDS=30

# ============================================================================
# CORS (Cross-Origin Resource Sharing) Configuration
# ============================================================================

# Allowed origins for CORS
# Default allows Angular frontend on localhost:4200
# For production, specify your frontend domain
# Format: Comma-separated list or JSON array
CORS_ORIGINS=["http://localhost:4200","http://localhost:3000","http://localhost:8080"]

# For production (example):
# CORS_ORIGINS=["https://yourdomain.com","https://app.yourdomain.com"]

# Allow all origins (NOT recommended for production):
# CORS_ORIGINS=["*"]

# ============================================================================
# Advanced Settings (Usually don't need to change)
# ============================================================================

# Enable debug mode (provides detailed error messages)
# WARNING: Only use in development, not in production
# DEBUG=false

# Number of worker processes for uvicorn
# Default: 1 (increase for production)
# WORKERS=1

# ============================================================================
# Notes
# ============================================================================
#
# 1. After changing .env, restart the backend server for changes to take effect
#
# 2. To use GPU acceleration:
#    - Ensure NVIDIA GPU drivers are installed
#    - Install CUDA toolkit (11.8 or higher)
#    - Install PyTorch with CUDA support
#    - Set WHISPER_DEVICE=cuda
#
# 3. Model downloads:
#    - Models are cached in ~/.cache/whisper/
#    - First use of a model will download it automatically
#    - Pre-download with: python scripts/download_whisper_model.py <model>
#
# 4. Database initialization:
#    - Run: python scripts/init_db.py
#    - Safe to run multiple times (won't delete data)
#
# 5. Frontend configuration:
#    - Frontend expects backend on port 8001
#    - If you change API_PORT, update frontend environment.ts
#    - Path: src/presentation/frontend/src/environments/environment.ts
