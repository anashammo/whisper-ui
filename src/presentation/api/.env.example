# ============================================================================
# Whisper Transcription API - Environment Configuration
# ============================================================================
# Copy this file to .env in the same directory:
#   cp .env.example .env
#
# This configuration is optimized for Docker deployment (recommended).
# For local development without Docker, see "Local Development" section below.
# ============================================================================

# ============================================================================
# PostgreSQL Database Configuration (Docker)
# ============================================================================
POSTGRES_USER=whisper
POSTGRES_PASSWORD=change_this_secure_password_in_production
POSTGRES_DB=whisper_db

# Database connection URL is automatically constructed in docker-compose.yml
# Format: postgresql://user:password@host:port/database

# --- Local Development (without Docker) ---
# Uncomment the line below and comment out POSTGRES_* variables above
# DATABASE_URL=sqlite:///./whisper_transcriptions.db

# ============================================================================
# Application Settings
# ============================================================================
APP_NAME="Whisper Transcription API"
APP_VERSION=1.0.0

# Debug mode (provides detailed error messages)
# WARNING: Only use in development, not in production
DEBUG=false

# Logging level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# ============================================================================
# API Server Configuration
# ============================================================================
# Host to bind (0.0.0.0 for Docker/external access, localhost for local only)
API_HOST=0.0.0.0

# API Port (internal container port, mapped in docker-compose.yml)
API_PORT=8001

# External port mapping (used by docker-compose.yml)
BACKEND_PORT=8001
FRONTEND_PORT=4200
POSTGRES_PORT=5533

# ============================================================================
# Whisper Model Configuration
# ============================================================================
# Default Whisper model for transcriptions
# Options: tiny, base, small, medium, large, turbo
#
# Model Comparison:
# - tiny:   ~75MB  download, ~1GB  VRAM (fastest, least accurate)
# - base:   ~150MB download, ~1GB  VRAM (recommended, good balance)
# - small:  ~500MB download, ~2GB  VRAM (better accuracy)
# - medium: ~1.5GB download, ~5GB  VRAM (high accuracy)
# - large:  ~3GB   download, ~10GB VRAM (best accuracy, slowest)
# - turbo:  ~3GB   download, ~6GB  VRAM (speed + accuracy optimized)
WHISPER_MODEL=base

# Device for Whisper (cuda for GPU, cpu for CPU-only)
# Docker: cuda (requires nvidia-docker runtime)
WHISPER_DEVICE=cuda

# Compute type (float16 for GPU, float32 for CPU)
WHISPER_COMPUTE_TYPE=float16

# ============================================================================
# Whisper Model Pre-loading Configuration (Docker only)
# ============================================================================
# Models to preload on container startup (space or comma-separated)
# Example: PRELOAD_MODELS=tiny,base,small
PRELOAD_MODELS=base

# Force re-download models even if cached (set to 1 to enable)
FORCE_DOWNLOAD=

# ============================================================================
# File Upload Settings
# ============================================================================
# Upload directory
# Docker: /app/uploads (mapped to volume)
# Local:  ./uploads (relative to project root)
UPLOAD_DIR=/app/uploads

# Maximum file size for uploads (in MB)
MAX_FILE_SIZE_MB=25

# Maximum audio duration (in seconds)
MAX_DURATION_SECONDS=30

# ============================================================================
# CORS Configuration
# ============================================================================
# Allowed origins for CORS (JSON array format)
# Docker: Include container names and external URLs
CORS_ORIGINS='["http://localhost","http://localhost:4200","http://frontend","*"]'

# For production, restrict to your domains:
# CORS_ORIGINS='["https://yourdomain.com","https://app.yourdomain.com"]'

# ============================================================================
# LLM Enhancement Configuration (Optional)
# ============================================================================
# Base URL for local LLM API (OpenAI-compatible)
# Docker: Use host.docker.internal to access host services
# Local:  Use localhost
LLM_BASE_URL=http://host.docker.internal:1234/v1

# LLM model name (depends on your LLM server)
# For Ollama: llama3, mistral, phi, etc.
# For LM Studio: model name as shown in LM Studio
LLM_MODEL=llama3

# Timeout for LLM requests (in seconds)
LLM_TIMEOUT_SECONDS=60

# LLM temperature (0.0-1.0, lower = more focused)
LLM_TEMPERATURE=0.3

# ============================================================================
# Ngrok Tunnel Configuration (Optional)
# ============================================================================
# Ngrok auth token for tunnel authentication
# Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken
NGROK_AUTHTOKEN=your_ngrok_authtoken_here

# Ngrok tunnels (requires reserved domains in your ngrok account):
# - Backend:  https://anas-hammo-whisper-backend.ngrok.dev
# - Frontend: https://anas-hammo-whisper-frontend.ngrok.dev
# - LLM:      https://anas-hammo-whisper-llm.ngrok.dev
#
# Ngrok Web Inspection UI (for debugging):
# - Backend:  http://localhost:4050
# - Frontend: http://localhost:4051
# - LLM:      http://localhost:4052

# ============================================================================
# Notes
# ============================================================================
#
# DOCKER DEPLOYMENT (Recommended):
#   1. Copy this file: cp .env.example .env
#   2. Update POSTGRES_PASSWORD and NGROK_AUTHTOKEN
#   3. Run: python scripts/docker/run.py
#
# LOCAL DEVELOPMENT (Without Docker):
#   1. Copy this file: cp .env.example .env
#   2. Comment out POSTGRES_* variables
#   3. Uncomment DATABASE_URL=sqlite:///./whisper_transcriptions.db
#   4. Change UPLOAD_DIR to ./uploads
#   5. Change LLM_BASE_URL to http://localhost:1234/v1
#   6. Run: python scripts/server/run_backend.py
#
# GPU ACCELERATION:
#   - Requires NVIDIA GPU with CUDA 12.8+ (for RTX 5090)
#   - Docker: nvidia-docker runtime required
#   - Verify: docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu22.04 nvidia-smi
#
# MODEL CACHING:
#   - Models cached in ~/.cache/huggingface/ (local) or huggingface-cache volume (Docker)
#   - First use downloads automatically
#   - Use PRELOAD_MODELS to download on container startup
#
# VOLUMES (Docker):
#   - postgres-data: PostgreSQL database files
#   - whisper-uploads: Uploaded audio files
#   - huggingface-cache: Whisper model files
#
