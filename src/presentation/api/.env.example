# ============================================================================
# Whisper Transcription API - Backend Environment Configuration (Local Dev)
# ============================================================================
# Copy this file to .env in the same directory for local development
# For Docker deployment, use .env.docker
# ============================================================================

# ============================================================================
# Application Settings
# ============================================================================
APP_NAME=Whisper Transcription API
APP_VERSION=1.0.0

# Debug mode (provides detailed error messages)
# WARNING: Only use in development, not in production
DEBUG=false

# Logging level
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# ============================================================================
# API Server Configuration
# ============================================================================
# Host to bind the API server to
# - 0.0.0.0: Listen on all network interfaces (default)
# - localhost or 127.0.0.1: Listen only on local machine
API_HOST=0.0.0.0

# Port for the API server
# Default: 8001 (NOT 8000, to avoid conflicts)
API_PORT=8001

# ============================================================================
# Database Configuration
# ============================================================================
# Database connection URL
# Default uses SQLite with local file
# Format: sqlite:///<path-to-database>
DATABASE_URL=sqlite:///./whisper_transcriptions.db

# For production, consider PostgreSQL:
# DATABASE_URL=postgresql://user:password@localhost/whisper_db

# ============================================================================
# Whisper Model Configuration
# ============================================================================
# Default Whisper model to use for transcriptions
# This is the fallback if user doesn't specify a model
# Options: tiny, base, small, medium, large, turbo
#
# Model Comparison:
# - tiny:   ~75MB  download, ~1GB  VRAM (fastest, least accurate)
# - base:   ~150MB download, ~1GB  VRAM (recommended, good balance)
# - small:  ~500MB download, ~2GB  VRAM (better accuracy)
# - medium: ~1.5GB download, ~5GB  VRAM (high accuracy)
# - large:  ~3GB   download, ~10GB VRAM (best accuracy, slowest)
# - turbo:  ~3GB   download, ~6GB  VRAM (speed + accuracy optimized)
WHISPER_MODEL=base

# Device to run Whisper model on
# Options: cuda, cpu
# - cuda: Use GPU acceleration (requires NVIDIA GPU with CUDA)
# - cpu: Use CPU (slower, but works without GPU)
WHISPER_DEVICE=cuda

# Compute type (float16 for GPU, float32 for CPU)
WHISPER_COMPUTE_TYPE=float16

# ============================================================================
# File Upload Settings
# ============================================================================
# Directory to store uploaded audio files
# Relative to project root for local dev
UPLOAD_DIR=./uploads

# Maximum file size for uploads (in MB)
MAX_FILE_SIZE_MB=25

# Maximum audio duration (in seconds)
# Note: Longer audio requires more VRAM and processing time
MAX_DURATION_SECONDS=30

# ============================================================================
# CORS (Cross-Origin Resource Sharing) Configuration
# ============================================================================
# Allowed origins for CORS
# Default allows Angular frontend on localhost:4200
# Format: JSON array
CORS_ORIGINS=["http://localhost:4200","http://localhost:3000","http://localhost:8080"]

# For production (example):
# CORS_ORIGINS=["https://yourdomain.com","https://app.yourdomain.com"]

# Allow all origins (NOT recommended for production):
# CORS_ORIGINS=["*"]

# ============================================================================
# LLM Enhancement Configuration (Optional)
# ============================================================================
# Base URL for local LLM API (OpenAI-compatible)
# Default: Ollama default endpoint
# For LM Studio, use: http://localhost:1234/v1
LLM_BASE_URL=http://localhost:11434/v1

# LLM model name to use for transcription enhancement
# For Ollama: llama3, mistral, phi, etc.
# For LM Studio: model name as shown in LM Studio
LLM_MODEL=llama3

# Timeout for LLM requests (in seconds)
# Default: 60 seconds
# Increase for slower models or longer transcriptions
LLM_TIMEOUT_SECONDS=60

# LLM temperature (0.0 - 1.0)
# Lower = more focused and deterministic
# Higher = more creative and varied
# Default: 0.3 (good for transcription enhancement)
LLM_TEMPERATURE=0.3

# ============================================================================
# Notes
# ============================================================================
#
# 1. After changing .env, restart the backend server for changes to take effect
#
# 2. To use GPU acceleration:
#    - Ensure NVIDIA GPU drivers are installed
#    - Install CUDA toolkit (12.8 or higher for RTX 5090)
#    - Install PyTorch with CUDA support
#    - Set WHISPER_DEVICE=cuda
#
# 3. Model downloads:
#    - Models are cached in ~/.cache/whisper/
#    - First use of a model will download it automatically
#    - Pre-download with: python scripts/setup/download_whisper_model.py <model>
#
# 4. Database initialization:
#    - Run: python scripts/setup/init_db.py
#    - Safe to run multiple times (won't delete data)
#
# 5. Frontend configuration:
#    - Frontend expects backend on port 8001
#    - If you change API_PORT, update frontend src/environments/environment.ts
#    - Frontend path: src/presentation/frontend/src/environments/environment.ts
#
