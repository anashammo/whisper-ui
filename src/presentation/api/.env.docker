# ============================================================================
# Whisper Transcription API - Backend Docker Environment Configuration
# ============================================================================
# Copy this file to .env in the same directory for Docker deployment
# This configuration is optimized for Docker containers
# ============================================================================

# ============================================================================
# PostgreSQL Database Configuration (Docker)
# ============================================================================
POSTGRES_USER=whisper
POSTGRES_PASSWORD=change_this_secure_password_in_production
POSTGRES_DB=whisper_db

# Database connection URL (for PostgreSQL in Docker)
# Automatically constructed from above variables in docker-compose.yml
# Format: postgresql://user:password@host:port/database
# DATABASE_URL=postgresql://whisper:password@postgres:5432/whisper_db

# ============================================================================
# Application Settings
# ============================================================================
APP_NAME="Whisper Transcription API"
APP_VERSION=1.0.0

# Debug mode (provides detailed error messages)
# WARNING: Only use in development, not in production
DEBUG=false

# Logging level
LOG_LEVEL=INFO

# ============================================================================
# API Server Configuration
# ============================================================================
# Host to bind (use 0.0.0.0 for Docker to accept external connections)
API_HOST=0.0.0.0

# API Port (internal container port, mapped in docker-compose.yml)
API_PORT=8001

# External port mapping (configured in docker-compose.yml)
BACKEND_PORT=8001
FRONTEND_PORT=4200

# ============================================================================
# Whisper Configuration
# ============================================================================
# Default Whisper model for transcriptions
# This is the fallback if user doesn't specify a model
WHISPER_MODEL=base

# Device for Whisper (cuda for GPU, cpu for CPU-only)
# Docker: cuda (requires nvidia-docker runtime)
WHISPER_DEVICE=cuda

# Compute type (float16 for GPU, float32 for CPU)
WHISPER_COMPUTE_TYPE=float16

# ============================================================================
# Whisper Model Pre-loading Configuration
# ============================================================================
# Models to preload on container startup (comma or space-separated)
# Options: tiny, base, small, medium, large, turbo
# Default: base (good balance of speed and accuracy)
# Example: PRELOAD_MODELS=tiny,base,small
PRELOAD_MODELS=base

# Force re-download models even if they exist in cache
# Set to any value to enable (e.g., FORCE_DOWNLOAD=1)
# Leave empty to use cached models
FORCE_DOWNLOAD=

# ============================================================================
# File Upload Settings
# ============================================================================
# Upload directory (inside container, mapped to Docker volume)
UPLOAD_DIR=/app/uploads

# Maximum file size for uploads (in MB)
MAX_FILE_SIZE_MB=25

# Maximum audio duration (in seconds)
MAX_DURATION_SECONDS=30

# ============================================================================
# CORS Configuration
# ============================================================================
# Allowed origins for CORS
# For Docker: Include both internal (container) and external (host) URLs
# Format: JSON array
CORS_ORIGINS='["http://localhost","http://localhost:4200","http://frontend","*"]'

# ============================================================================
# LLM Enhancement Configuration (Optional)
# ============================================================================
# Base URL for local LLM API (OpenAI-compatible)
# For Docker: If Ollama is running on host, use host.docker.internal
# Example: http://host.docker.internal:11434/v1
# If Ollama is in Docker Compose, use service name: http://ollama:11434/v1
LLM_BASE_URL=http://host.docker.internal:1234/v1

# LLM model name
LLM_MODEL=openai/gpt-oss-20b

# LLM timeout (in seconds)
LLM_TIMEOUT_SECONDS=60

# LLM temperature (0.0 - 1.0)
LLM_TEMPERATURE=0.3

# ============================================================================
# Docker-Specific Notes
# ============================================================================
#
# 1. Database:
#    - Uses PostgreSQL (configured above)
#    - Data persists in postgres-data volume
#    - Separate volume for logs: postgres-data:/var/lib/postgresql/data
#
# 2. Model Caching:
#    - Models are cached in whisper-cache Docker volume
#    - Persists across container restarts and rebuilds
#    - First run will download models (may take time)
#    - Use PRELOAD_MODELS to download on startup
#
# 3. GPU Access:
#    - Requires nvidia-docker runtime installed on host
#    - GPU must be visible to Docker: docker run --gpus all
#    - Verify with: docker run --rm --gpus all nvidia/cuda:12.8.0-base-ubuntu22.04 nvidia-smi
#
# 4. Networking:
#    - Frontend and backend communicate via Docker network
#    - API URL in frontend: http://backend:8001/api/v1 (internal)
#    - External access: http://localhost:8001 (backend)
#
# 5. Volumes:
#    - postgres-data: PostgreSQL database files + logs
#    - whisper-uploads: Uploaded audio files
#    - whisper-cache: Whisper model files (~/.cache/whisper)
#
# 6. LLM Service:
#    - Not included in Docker Compose (run separately)
#    - Use host.docker.internal to access host services from container
#    - Ensure LLM service (Ollama/LM Studio) is running on host
#
# 7. Environment Variable Loading:
#    - This file should be copied to src/presentation/api/.env
#    - Docker Compose will load it automatically via env_file directive
#    - Changes require container restart: docker-compose restart backend
#
